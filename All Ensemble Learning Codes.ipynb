{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ed23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://scikit-learn.org/stable/modules/ensemble.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots \n",
    "'https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_regressor.html#sphx-glr-auto-examples-ensemble-plot-voting-regressor-py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3debb7e2",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e7a38",
   "metadata": {},
   "source": [
    "Here's an example code snippet for ensemble learning using a technique called \"voting\" in Python\n",
    "\n",
    "In this example, we use the VotingClassifier from the sklearn.ensemble module to create an ensemble model. Three different classifiers, namely DecisionTreeClassifier, SVC (Support Vector Classifier), and LogisticRegression, are defined as individual models. These models are then combined into an ensemble using the VotingClassifier, specifying the voting strategy as 'hard', which means the majority class prediction is selected.\n",
    "\n",
    "The ensemble is trained using the fit() function on the training data. Predictions are made on the test set using the predict() method of the ensemble model. Finally, the accuracy of the ensemble predictions is evaluated using the accuracy_score() function from sklearn.metrics.\n",
    "\n",
    "Note that this is just a basic example, and there are various other techniques and strategies for ensemble learning, such as weighted voting, stacking, and boosting, which can be explored depending on the problem and data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315bf27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define individual classifiers\n",
    "classifier_1 = DecisionTreeClassifier()\n",
    "classifier_2 = SVC(probability=True)\n",
    "classifier_3 = LogisticRegression()\n",
    "\n",
    "# Create an ensemble using voting\n",
    "ensemble = VotingClassifier(estimators=[('dt', classifier_1), ('svm', classifier_2), ('lr', classifier_3)], voting='hard')\n",
    "\n",
    "# Fit the ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ensemble Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da346ed",
   "metadata": {},
   "source": [
    "### Stacking Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b6740",
   "metadata": {},
   "source": [
    "Stacking is an ensemble learning technique that involves training multiple models and using another model, called a meta-model, to combine their predictions. Here's an example code snippet for stacking ensemble using scikit-learn library in Python.\n",
    "\n",
    "In this example, we use the StackingClassifier from the sklearn.ensemble module to create a stacking ensemble model. Three different base classifiers, namely DecisionTreeClassifier, SVC (Support Vector Classifier), and LogisticRegression, are defined. These base classifiers are specified as estimators in the StackingClassifier.\n",
    "\n",
    "A meta-classifier, DecisionTreeClassifier in this case, is also defined as the final estimator. It is trained to combine the predictions of the base classifiers.\n",
    "\n",
    "The stacking ensemble is trained using the fit() function on the training data. Predictions are made on the test set using the predict() method of the ensemble model. Finally, the accuracy of the ensemble predictions is evaluated using the accuracy_score() function from sklearn.metrics.\n",
    "\n",
    "Note that in stacking, the base classifiers can be trained on the entire training set, while the meta-classifier is typically trained on a holdout set or using cross-validation to avoid overfitting.\n",
    "\n",
    "This is a basic example to demonstrate the stacking ensemble technique, and you can further explore and customize the approach based on your specific problem and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define individual base classifiers\n",
    "classifier_1 = DecisionTreeClassifier()\n",
    "classifier_2 = SVC(probability=True)\n",
    "classifier_3 = LogisticRegression()\n",
    "\n",
    "# Define the meta-model\n",
    "meta_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Create the stacking ensemble\n",
    "ensemble = StackingClassifier(estimators=[('dt', classifier_1), ('svm', classifier_2), ('lr', classifier_3)], \n",
    "                              final_estimator=meta_classifier)\n",
    "\n",
    "# Fit the stacking ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ensemble Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce539514",
   "metadata": {},
   "source": [
    "### Bagging Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01212fd4",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble learning technique that involves training multiple models on different subsets of the training data and combining their predictions. Here's an example code snippet for bagging ensemble using scikit-learn library in Python:\n",
    "\n",
    "In this example, we use the BaggingClassifier from the sklearn.ensemble module to create a bagging ensemble model. A base classifier, in this case, DecisionTreeClassifier, is defined as the base estimator for the bagging ensemble.\n",
    "\n",
    "The n_estimators parameter is set to 10, indicating that the ensemble will consist of 10 individual classifiers. Each classifier is trained on a randomly selected subset of the training data, created using the bootstrap sampling technique.\n",
    "\n",
    "The bagging ensemble is trained using the fit() function on the training data. Predictions are made on the test set using the predict() method of the ensemble model. Finally, the accuracy of the ensemble predictions is evaluated using the accuracy_score() function from sklearn.metrics.\n",
    "\n",
    "Note that bagging can be used with various base classifiers and other techniques like random feature selection (random subspaces) to further diversify the ensemble. Additionally, bagging can be applied to other types of models, such as regression or clustering algorithms, with appropriate modifications.\n",
    "\n",
    "This is a basic example to demonstrate the bagging ensemble technique, and you can further explore and customize the approach based on your specific problem and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4708de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base classifier\n",
    "base_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Create the bagging ensemble\n",
    "ensemble = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Fit the bagging ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ensemble Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423db70a",
   "metadata": {},
   "source": [
    "### Bagging ensemble code with svm as base estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dffb74",
   "metadata": {},
   "source": [
    "Bagging ensemble using Support Vector Machines (SVM) as the base estimator in scikit-learn library in Python\n",
    "\n",
    "In this example, we use the BaggingClassifier from the sklearn.ensemble module to create a bagging ensemble model. The base classifier is set as SVC (Support Vector Classifier) with the kernel='linear' parameter.\n",
    "\n",
    "The n_estimators parameter is set to 10, indicating that the ensemble will consist of 10 individual classifiers. Each classifier is trained on a randomly selected subset of the training data, created using the bootstrap sampling technique.\n",
    "\n",
    "The bagging ensemble is trained using the fit() function on the training data. Predictions are made on the test set using the predict() method of the ensemble model. Finally, the accuracy of the ensemble predictions is evaluated using the accuracy_score() function from sklearn.metrics.\n",
    "\n",
    "Feel free to adjust the parameters, such as the kernel or the number of estimators, based on your specific problem and data requirements.\n",
    "\n",
    "Remember to import the necessary libraries (sklearn.ensemble, sklearn.svm, sklearn.datasets, sklearn.model_selection, sklearn.metrics, etc.) before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base classifier\n",
    "base_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Create the bagging ensemble\n",
    "ensemble = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Fit the bagging ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ensemble Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4078c0",
   "metadata": {},
   "source": [
    "### Bagging ensemble code using Logistic Regression as the base estimator in Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb605f7",
   "metadata": {},
   "source": [
    "In this code, we first generate a synthetic classification dataset using the make_classification function from scikit-learn. Then, we split the dataset into training and testing sets.\n",
    "\n",
    "Next, we create an instance of the Logistic Regression classifier as the base estimator. We then initialize the BaggingClassifier, passing the base estimator and the number of estimators (10 in this example). The random_state parameter ensures reproducibility of results.\n",
    "\n",
    "We train the bagging ensemble by calling the fit method on the bagging object, using the training data.\n",
    "\n",
    "After training, we make predictions on the test set using the predict method and calculate the accuracy of the bagging ensemble using the score method.\n",
    "\n",
    "Feel free to modify the code as per your specific requirements or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a base logistic regression estimator\n",
    "base_estimator = LogisticRegression()\n",
    "\n",
    "# Create the bagging classifier\n",
    "bagging = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the bagging ensemble\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the bagging ensemble\n",
    "accuracy = bagging.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99112998",
   "metadata": {},
   "source": [
    "### Bagging ensemble code with Random forest as base estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e9ff1",
   "metadata": {},
   "source": [
    "bagging ensemble code using Random Forest as the base estimator in Python:\n",
    "    \n",
    "In this code, we first generate a synthetic classification dataset using the make_classification function from scikit-learn. Then, we split the dataset into training and testing sets.\n",
    "\n",
    "Next, we create an instance of the Random Forest classifier as the base estimator. We then initialize the BaggingClassifier, passing the base estimator, the number of estimators (10 in this example), and the random_state parameter for reproducibility.\n",
    "\n",
    "We train the bagging ensemble by calling the fit method on the bagging object, using the training data.\n",
    "\n",
    "After training, we make predictions on the test set using the predict method and calculate the accuracy of the bagging ensemble using the score method.\n",
    "\n",
    "Feel free to modify the code as per your specific requirements or dataset.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6adead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a base random forest estimator\n",
    "base_estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create the bagging classifier\n",
    "bagging = BaggingClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the bagging ensemble\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the bagging ensemble\n",
    "accuracy = bagging.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034d3e3",
   "metadata": {},
   "source": [
    "### Boosting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0766f",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique that combines multiple weak learners (models with low predictive power) to create a strong learner. Here's an example code snippet for boosting ensemble using scikit-learn library in Python:\n",
    "\n",
    "In this example, we use the AdaBoostClassifier from the sklearn.ensemble module to create a boosting ensemble model. A base classifier, in this case, DecisionTreeClassifier with max_depth=1, is defined as the weak learner for the boosting ensemble.\n",
    "\n",
    "The n_estimators parameter is set to 50, indicating that the ensemble will consist of 50 weak learners. Each weak learner is trained sequentially, with each subsequent learner focusing more on the samples that were misclassified by the previous learners.\n",
    "\n",
    "The boosting ensemble is trained using the fit() function on the training data. Predictions are made on the test set using the predict() method of the ensemble model. Finally, the accuracy of the ensemble predictions is evaluated using the accuracy_score() function from sklearn.metrics.\n",
    "\n",
    "Note that boosting can be used with various base classifiers and other techniques like gradient boosting (e.g., GradientBoostingClassifier) or XGBoost to further improve the performance of the ensemble.\n",
    "\n",
    "This is a basic example to demonstrate the boosting ensemble technique, and you can further explore and customize the approach based on your specific problem and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea147365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base classifier\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create the boosting ensemble\n",
    "ensemble = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "\n",
    "# Fit the boosting ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Ensemble Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5019a",
   "metadata": {},
   "source": [
    "### Boosting ensemble code with svm as base estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990344e",
   "metadata": {},
   "source": [
    "boosting ensemble code using SVM (Support Vector Machine) as the base estimator in Python:\n",
    "    \n",
    "In this code, we first generate a synthetic classification dataset using the make_classification function from scikit-learn. Then, we split the dataset into training and testing sets.\n",
    "\n",
    "Next, we create an instance of the SVM classifier as the base estimator. We set probability=True to enable probability estimates, which are required for boosting.\n",
    "\n",
    "We then initialize the AdaBoostClassifier, passing the base estimator, the number of estimators (10 in this example), and the random_state parameter for reproducibility.\n",
    "\n",
    "We train the boosting ensemble by calling the fit method on the boosting object, using the training data.\n",
    "\n",
    "After training, we make predictions on the test set using the predict method and calculate the accuracy of the boosting ensemble using the score method.\n",
    "\n",
    "Feel free to modify the code as per your specific requirements or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a base SVM estimator\n",
    "base_estimator = SVC(probability=True, random_state=42)\n",
    "\n",
    "# Create the boosting classifier\n",
    "boosting = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the boosting ensemble\n",
    "boosting.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = boosting.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the boosting ensemble\n",
    "accuracy = boosting.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
